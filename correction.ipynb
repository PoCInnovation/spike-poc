{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e24dd1",
   "metadata": {},
   "source": [
    "## Exercise 1 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc30b3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2 as b2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def run_lif_simulation(input_current, tau_m, refractory_period, duration=100*b2.ms):\n",
    "    \"\"\"Runs and plots LIF simulation for given parameters.\"\"\"\n",
    "    b2.start_scope()\n",
    "\n",
    "    V_rest = -65 * b2.mV\n",
    "    V_reset = -65 * b2.mV\n",
    "    V_th = -50 * b2.mV\n",
    "    R_m = 100 * b2.Mohm\n",
    "\n",
    "    lif_eqs = '''\n",
    "    dv/dt = (-(v - V_rest) + R_m * I) / tau_m : volt (unless refractory)\n",
    "    I : amp\n",
    "    '''\n",
    "\n",
    "    G = b2.NeuronGroup(1, lif_eqs, threshold='v > V_th', reset='v = V_reset',\n",
    "                       refractory=refractory_period, method='exact')\n",
    "\n",
    "    G.v = V_rest\n",
    "\n",
    "    G.I = input_current\n",
    "\n",
    "    spike_monitor = b2.SpikeMonitor(G)\n",
    "    state_monitor = b2.StateMonitor(G, 'v', record=0)\n",
    "\n",
    "    b2.run(duration)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(state_monitor.t / b2.ms, state_monitor.v[0] / b2.mV, label='Membrane Potential')\n",
    "    plt.axhline(V_th / b2.mV, color='red', linestyle='--', label='Threshold V_th')\n",
    "    plt.axhline(V_rest / b2.mV, color='gray', linestyle=':', label='Resting V_rest')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Potential (mV)')\n",
    "    plt.title(f'LIF Neuron (I={input_current}, tau={tau_m}, refractory={refractory_period})')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    if spike_monitor.num_spikes > 0:\n",
    "        plt.plot(spike_monitor.t / b2.ms, spike_monitor.i, '.k', label='Spikes')\n",
    "    else:\n",
    "        plt.plot([], [], '.k', label='Spikes')\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Neuron Index')\n",
    "    plt.yticks([])\n",
    "    plt.title('Spike Output')\n",
    "    plt.grid(True)\n",
    "    plt.ylim(-0.5, 0.5)\n",
    "    plt.xlim(0, duration / b2.ms)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    num_spikes = spike_monitor.num_spikes\n",
    "    firing_rate = num_spikes / (duration / b2.second)\n",
    "    print(f\"Number of spikes: {num_spikes}\")\n",
    "    print(f\"Approximate firing rate: {firing_rate:.2f} Hz\")\n",
    "    if num_spikes > 0:\n",
    "        print(f\"Spike times: {spike_monitor.t / b2.ms} ms\")\n",
    "\n",
    "print(\"--- Testing Input Current = 150 pA ---\")\n",
    "run_lif_simulation(input_current=150 * b2.pA, tau_m=10*b2.ms, refractory_period=5*b2.ms)\n",
    "\n",
    "print(\"\\n--- Testing tau_m = 20 ms ---\")\n",
    "run_lif_simulation(input_current=200 * b2.pA, tau_m=20*b2.ms, refractory_period=5*b2.ms)\n",
    "\n",
    "print(\"\\n--- Testing refractory = 0 ms ---\")\n",
    "run_lif_simulation(input_current=200 * b2.pA, tau_m=10*b2.ms, refractory_period=0*b2.ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb39bb49",
   "metadata": {},
   "source": [
    "## Exercise 2 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040191c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2 as b2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from random import sample\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "def run_network_simulation(num_inputs=50, num_outputs=10, input_rate=20*b2.Hz,\n",
    "                           min_weight_val=0.5, max_weight_val=1.5,\n",
    "                           connection_rule='synapses.connect()',\n",
    "                           duration=200*b2.ms):\n",
    "    \"\"\"Runs and plots the simple feedforward network simulation.\"\"\"\n",
    "    b2.start_scope()\n",
    "\n",
    "    tau_m = 10 * b2.ms\n",
    "    V_rest = -65 * b2.mV\n",
    "    V_reset = -65 * b2.mV\n",
    "    V_th = -50 * b2.mV\n",
    "    lif_eqs = '''\n",
    "    dv/dt = -(v - V_rest) / tau_m : volt (unless refractory)\n",
    "    '''\n",
    "\n",
    "    input_group = b2.PoissonGroup(num_inputs, rates=input_rate)\n",
    "\n",
    "    output_group = b2.NeuronGroup(num_outputs, lif_eqs, threshold='v > V_th',\n",
    "                                  reset='v = V_reset', refractory=5*b2.ms, method='exact')\n",
    "    output_group.v = V_rest\n",
    "\n",
    "    synapse_model = 'w : volt'\n",
    "    on_pre_eq = 'v_post += w'\n",
    "    synapses = b2.Synapses(input_group, output_group, model=synapse_model, on_pre=on_pre_eq)\n",
    "\n",
    "    try:\n",
    "        exec(connection_rule)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing connection rule '{connection_rule}': {e}\")\n",
    "        return\n",
    "\n",
    "    if len(synapses) > 0:\n",
    "        synapses.w = np.random.uniform(min_weight_val, max_weight_val, size=len(synapses)) * b2.mV\n",
    "    else:\n",
    "        print(\"Warning: No synapses were created based on the connection rule.\")\n",
    "\n",
    "    input_spike_mon = b2.SpikeMonitor(input_group, name='InputSpikes')\n",
    "    output_spike_mon = b2.SpikeMonitor(output_group, name='OutputSpikes')\n",
    "\n",
    "    print(f\"Running simulation (duration={duration}, N_in={num_inputs}, N_out={num_outputs}, rate={input_rate}, weights=[{min_weight_val},{max_weight_val}], connect='{connection_rule}')\")\n",
    "    b2.run(duration)\n",
    "    print(\"Simulation complete.\")\n",
    "\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.subplot(2, 1, 1)\n",
    "    if input_spike_mon.num_spikes > 0:\n",
    "        plt.plot(input_spike_mon.t / b2.ms, input_spike_mon.i, '.k', markersize=2)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Input Neuron Index')\n",
    "    plt.title(f'Input Layer Spikes ({num_inputs} Poisson Neurons @ {input_rate})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, duration / b2.ms)\n",
    "    plt.ylim(-1, num_inputs)\n",
    "\n",
    "    plt.subplot(2, 1, 2)\n",
    "    if output_spike_mon.num_spikes > 0:\n",
    "        plt.plot(output_spike_mon.t / b2.ms, output_spike_mon.i, '.r', markersize=4)\n",
    "    plt.xlabel('Time (ms)')\n",
    "    plt.ylabel('Output Neuron Index')\n",
    "    plt.title(f'Output Layer Spikes ({num_outputs} LIF Neurons)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, duration / b2.ms)\n",
    "    plt.ylim(-1, num_outputs)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Total input spikes: {input_spike_mon.num_spikes}\")\n",
    "    print(f\"Total output spikes: {output_spike_mon.num_spikes}\")\n",
    "\n",
    "print(\"--- Testing Increased Synaptic Strength ---\")\n",
    "run_network_simulation(min_weight_val=2.0, max_weight_val=3.0)\n",
    "\n",
    "print(\"\\n--- Testing Inhibitory Synaptic Strength ---\")\n",
    "run_network_simulation(min_weight_val=-1.5, max_weight_val=-0.5)\n",
    "\n",
    "print(\"\\n--- Testing Sparse Connectivity (p=0.1) ---\")\n",
    "run_network_simulation(connection_rule='synapses.connect(p=0.1)')\n",
    "\n",
    "print(\"\\n--- Testing Higher Input Rate (40 Hz) ---\")\n",
    "run_network_simulation(input_rate=40*b2.Hz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a64da43",
   "metadata": {},
   "source": [
    "## Exercise 3 Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d013ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "class GridWorldEnv:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.goal_pos = (size - 1, size - 1)\n",
    "        self.hole_pos = (1, 1)\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_delta = {\n",
    "            0: (-1, 0),\n",
    "            1: (1, 0),\n",
    "            2: (0, -1),\n",
    "            3: (0, 1)\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        delta = self.action_delta[action]\n",
    "        current_r, current_c = self.agent_pos\n",
    "        next_r, next_c = current_r + delta[0], current_c + delta[1]\n",
    "\n",
    "        if not (0 <= next_r < self.size and 0 <= next_c < self.size):\n",
    "            next_r, next_c = current_r, current_c\n",
    "\n",
    "        self.agent_pos = (next_r, next_c)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        elif self.agent_pos == self.hole_pos:\n",
    "            reward = -10.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.size, self.size), '_', dtype=str)\n",
    "        grid[self.goal_pos] = 'G'\n",
    "        grid[self.hole_pos] = 'H'\n",
    "        grid[self.agent_pos] = 'A'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(\"-\" * (self.size * 2 - 1))\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.actions = getattr(env, 'actions', [0, 1, 2, 3])\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get(state, {}).get(action, 0.0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, a) for a in self.actions]\n",
    "            max_q = np.max(q_values)\n",
    "            if all(q == q_values[0] for q in q_values):\n",
    "                 best_actions = self.actions\n",
    "            else:\n",
    "                 best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        old_q = self.get_q_value(state, action)\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "            next_max_q = np.max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "            td_target = reward + self.gamma * next_max_q\n",
    "        td_error = td_target - old_q\n",
    "        new_q = old_q + self.alpha * td_error\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {act: 0.0 for act in self.actions}\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "def train_agent(env, agent_params, num_episodes=5000, max_steps_per_episode=100, verbose=True):\n",
    "    \"\"\"Trains a Q-learning agent with given parameters.\"\"\"\n",
    "    agent = QLearningAgent(env, **agent_params)\n",
    "    rewards_per_episode = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"\\n--- Training with params: {agent_params} ---\")\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        for step in range(max_steps_per_episode):\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.update_q_table(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        agent.update_epsilon()\n",
    "        rewards_per_episode.append(total_reward)\n",
    "\n",
    "        if verbose and (episode + 1) % (num_episodes // 10) == 0:\n",
    "            avg_reward = np.mean(rewards_per_episode[-(num_episodes // 10):])\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward (last {num_episodes // 10}): {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(f\"Training finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    window_size = 100\n",
    "    if len(rewards_per_episode) >= window_size:\n",
    "        smoothed_rewards = np.convolve(rewards_per_episode, np.ones(window_size)/window_size, mode='valid')\n",
    "        plt.plot(smoothed_rewards, label=f\"Params: {agent_params}\")\n",
    "        plt.xlabel(f'Episode (Moving Average over {window_size} episodes)')\n",
    "    else:\n",
    "        plt.plot(rewards_per_episode, label=f\"Params: {agent_params}\")\n",
    "        plt.xlabel('Episode')\n",
    "    plt.title('Episode Rewards over Time')\n",
    "    plt.ylabel('Total Reward')\n",
    "    plt.grid(True)\n",
    "    plt.legend(fontsize='small')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Learned Policy:\")\n",
    "    action_arrows = {0: '^', 1: 'v', 2: '<', 3: '>'}\n",
    "    policy_grid = np.full((env.size, env.size), ' ', dtype=str)\n",
    "    policy_grid[env.goal_pos] = 'G'\n",
    "    policy_grid[env.hole_pos] = 'H'\n",
    "    for r in range(env.size):\n",
    "        for c in range(env.size):\n",
    "            state = (r, c)\n",
    "            if state != env.goal_pos and state != env.hole_pos:\n",
    "                if state in agent.q_table:\n",
    "                    q_values = [agent.get_q_value(state, a) for a in agent.actions]\n",
    "                    best_action = agent.actions[np.argmax(q_values)]\n",
    "                    policy_grid[r, c] = action_arrows[best_action]\n",
    "                else:\n",
    "                     policy_grid[r, c] = '.'\n",
    "    print(\"\\n\".join(\" \".join(row) for row in policy_grid))\n",
    "    print(\"-\" * (env.size * 2 - 1))\n",
    "\n",
    "grid_env = GridWorldEnv(size=4)\n",
    "\n",
    "base_params = {'alpha': 0.1, 'gamma': 0.99, 'epsilon': 1.0, 'epsilon_decay': 0.995, 'epsilon_min': 0.01}\n",
    "train_agent(grid_env, base_params)\n",
    "\n",
    "high_alpha_params = base_params.copy()\n",
    "high_alpha_params['alpha'] = 0.9\n",
    "train_agent(grid_env, high_alpha_params)\n",
    "\n",
    "low_gamma_params = base_params.copy()\n",
    "low_gamma_params['gamma'] = 0.1\n",
    "train_agent(grid_env, low_gamma_params)\n",
    "\n",
    "fast_decay_params = base_params.copy()\n",
    "fast_decay_params['epsilon_decay'] = 0.9\n",
    "train_agent(grid_env, fast_decay_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724bb4fe",
   "metadata": {},
   "source": [
    "## Exercise 4 Solution\n",
    "\n",
    "This solution demonstrates **Part 1** of Exercise 4: Changing `num_outputs` in the SNN setup.\n",
    "It requires re-running the SNN setup, environment definition, agent creation, and training loop with the modified parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0296b7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2 as b2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.q_table = {}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.actions = getattr(env, 'actions', [])\n",
    "        if not self.actions:\n",
    "             print(\"Warning: Actions not provided by env, defaulting potentially incorrectly.\")\n",
    "             self.actions = [0, 1]\n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get(state, {}).get(action, 0.0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            q_values = [self.get_q_value(state, a) for a in self.actions]\n",
    "            max_q = np.max(q_values)\n",
    "            if all(q == q_values[0] for q in q_values):\n",
    "                 best_actions = self.actions\n",
    "            else:\n",
    "                 best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            if not best_actions: \n",
    "                return np.random.choice(self.actions)\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        old_q = self.get_q_value(state, action)\n",
    "        if done:\n",
    "            td_target = reward\n",
    "        else:\n",
    "             if not self.actions:\n",
    "                  next_max_q = 0.0\n",
    "             else:\n",
    "                  next_max_q = np.max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "             td_target = reward + self.gamma * next_max_q\n",
    "            \n",
    "        td_error = td_target - old_q\n",
    "        new_q = old_q + self.alpha * td_error\n",
    "        if state not in self.q_table:\n",
    "             self.q_table[state] = {act: 0.0 for act in self.actions}\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "def setup_snn(num_inputs_snn, num_outputs_snn, conn_prob, weight_min, weight_max):\n",
    "    b2.start_scope()\n",
    "    tau_m = 10 * b2.ms\n",
    "    V_rest = -65 * b2.mV\n",
    "    V_reset = -65 * b2.mV\n",
    "    V_th = -50 * b2.mV\n",
    "    lif_eqs = 'dv/dt = -(v - V_rest) / tau_m : volt (unless refractory)'\n",
    "\n",
    "    input_group = b2.PoissonGroup(num_inputs_snn, rates=0*b2.Hz, name='input_layer')\n",
    "    output_group = b2.NeuronGroup(num_outputs_snn, lif_eqs, threshold='v > V_th',\n",
    "                                  reset='v = V_reset', refractory=5*b2.ms, method='exact',\n",
    "                                  name='output_layer')\n",
    "    output_group.v = V_rest\n",
    "    synapses = b2.Synapses(input_group, output_group, 'w : volt', on_pre='v_post += w',\n",
    "                             name='synapses')\n",
    "    synapses.connect(p=conn_prob)\n",
    "    if len(synapses) > 0:\n",
    "        synapses.w = np.random.uniform(weight_min, weight_max, size=len(synapses)) * b2.mV\n",
    "\n",
    "    output_spike_mon = b2.SpikeMonitor(output_group, name='output_spikes')\n",
    "    snn_net = b2.Network(b2.collect())\n",
    "    snn_net.store('initial_snn')\n",
    "    print(f\"SNN Setup: Input={num_inputs_snn}, Output={num_outputs_snn}, Synapses={len(synapses)}\")\n",
    "    return snn_net\n",
    "\n",
    "def define_patterns(num_inputs_pat, base_rate, high_rate):\n",
    "    pattern_A_rates = np.zeros(num_inputs_pat) * b2.Hz\n",
    "    pattern_A_rates[:num_inputs_pat // 2] = high_rate * b2.Hz\n",
    "    pattern_A_rates[num_inputs_pat // 2:] = base_rate * b2.Hz\n",
    "\n",
    "    pattern_B_rates = np.zeros(num_inputs_pat) * b2.Hz\n",
    "    pattern_B_rates[:num_inputs_pat // 2] = base_rate * b2.Hz\n",
    "    pattern_B_rates[num_inputs_pat // 2:] = high_rate * b2.Hz\n",
    "\n",
    "    patterns = {'A': pattern_A_rates, 'B': pattern_B_rates}\n",
    "    pattern_labels = {'A': 0, 'B': 1}\n",
    "    print(\"Spike patterns defined.\")\n",
    "    return patterns, pattern_labels\n",
    "\n",
    "class SNNPatternEnv:\n",
    "    def __init__(self, snn_network, patterns, pattern_labels, duration):\n",
    "        self.snn_network = snn_network\n",
    "        self.patterns = patterns\n",
    "        self.pattern_labels = pattern_labels\n",
    "        self.duration = duration\n",
    "        self.current_pattern_name = None\n",
    "        self.actions = list(set(pattern_labels.values()))\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_pattern_name = np.random.choice(list(self.patterns.keys()))\n",
    "        input_rates = self.patterns[self.current_pattern_name]\n",
    "        self.snn_network.restore('initial_snn')\n",
    "        if 'input_layer' in self.snn_network.objects_by_name:\n",
    "             self.snn_network['input_layer'].rates = input_rates\n",
    "        else:\n",
    "             print(\"Error: 'input_layer' not found in network objects during reset.\")\n",
    "             \n",
    "        self.snn_network.run(self.duration, report='off')\n",
    "        state = self._get_snn_state()\n",
    "        return state\n",
    "\n",
    "    def _get_snn_state(self):\n",
    "        spike_monitor = self.snn_network['output_spikes']\n",
    "        num_output_neurons = 0\n",
    "        if 'output_layer' in self.snn_network.objects_by_name:\n",
    "            num_output_neurons = self.snn_network['output_layer'].N\n",
    "        else:\n",
    "            print(\"Error: 'output_layer' not found in network objects for state calculation.\")\n",
    "            return tuple()\n",
    "\n",
    "        rates = np.zeros(num_output_neurons)\n",
    "        duration_sec = self.duration / b2.second\n",
    "        if duration_sec > 0 and spike_monitor.num_spikes > 0:\n",
    "            neuron_indices, counts = np.unique(spike_monitor.i, return_counts=True)\n",
    "            valid_indices = neuron_indices < num_output_neurons\n",
    "            rates[neuron_indices[valid_indices]] = counts[valid_indices] / duration_sec\n",
    "\n",
    "        bins = [-np.inf, 10, 30, np.inf]\n",
    "        discretized_rates = tuple(np.digitize(rates, bins))\n",
    "        return discretized_rates\n",
    "\n",
    "    def step(self, action):\n",
    "        correct_action = self.pattern_labels[self.current_pattern_name]\n",
    "        if action == correct_action:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -1.0\n",
    "        done = True\n",
    "        next_state = self._get_snn_state()\n",
    "        return next_state, reward, done\n",
    "\n",
    "def train_snn_agent(snn_env, agent_params, num_episodes=3000, verbose=True):\n",
    "    agent = QLearningAgent(snn_env, **agent_params)\n",
    "    rewards_per_episode_snn = []\n",
    "    history = []\n",
    "    print(f\"\\n--- Starting SNN+RL training ({num_episodes} episodes) ---\")\n",
    "    print(f\"Agent Params: {agent_params}\")\n",
    "    start_time_snn = time.time()\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = snn_env.reset()\n",
    "        if not isinstance(state, tuple):\n",
    "             print(f\"Error: Invalid state received from env.reset() at episode {episode+1}: {state}\")\n",
    "             break\n",
    "             \n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done = snn_env.step(action)\n",
    "\n",
    "        correct_action = snn_env.pattern_labels[snn_env.current_pattern_name]\n",
    "        history.append({'pattern': snn_env.current_pattern_name, 'state': state, 'chosen': action, 'correct': correct_action, 'reward': reward})\n",
    "\n",
    "        agent.update_q_table(state, action, reward, next_state, done)\n",
    "        agent.update_epsilon()\n",
    "        rewards_per_episode_snn.append(reward)\n",
    "\n",
    "        if verbose and (episode + 1) % (num_episodes // 10) == 0:\n",
    "            recent_history = history[-(num_episodes // 10):]\n",
    "            if len(recent_history) > 0:\n",
    "               recent_accuracy = sum(1 for h in recent_history if h['chosen'] == h['correct']) / len(recent_history)\n",
    "            else:\n",
    "               recent_accuracy = 0.0\n",
    "            print(f\"Episode {episode + 1}/{num_episodes} | Recent Acc: {recent_accuracy:.3f} | Epsilon: {agent.epsilon:.3f} | Q-States: {len(agent.q_table)}\")\n",
    "\n",
    "    end_time_snn = time.time()\n",
    "    if num_episodes > 0 and len(history)>0:\n",
    "       total_accuracy = sum(1 for h in history if h['chosen'] == h['correct']) / len(history)\n",
    "    else:\n",
    "        total_accuracy = 0.0\n",
    "    print(f\"SNN+RL Training finished in {end_time_snn - start_time_snn:.2f} seconds.\")\n",
    "    print(f\"Overall Accuracy: {total_accuracy:.3f}\")\n",
    "    print(f\"Final Q-Table size: {len(agent.q_table)} states encountered.\")\n",
    "\n",
    "    # Plotting Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    window_size = 100\n",
    "    accuracy_history = [1 if h['chosen'] == h['correct'] else 0 for h in history]\n",
    "    if len(accuracy_history) >= window_size:\n",
    "        smoothed_accuracy = np.convolve(accuracy_history, np.ones(window_size)/window_size, mode='valid')\n",
    "        plot_indices = np.arange(window_size - 1, len(accuracy_history))\n",
    "        plt.plot(plot_indices, smoothed_accuracy)\n",
    "        plt.xlabel(f'Episode (Smoothed over {window_size})')\n",
    "    elif len(accuracy_history) > 0:\n",
    "        plt.plot(accuracy_history)\n",
    "        plt.xlabel('Episode')\n",
    "\n",
    "    plt.title(f'Accuracy over Time (SNN+RL) - Params: {agent_params}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.ylim(-0.05, 1.05)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "NUM_INPUTS = 20\n",
    "PATTERN_DURATION = 100 * b2.ms\n",
    "BASE_RATE = 10\n",
    "HIGH_RATE = 50\n",
    "CONN_PROB = 0.5\n",
    "WEIGHT_MIN = 0.5\n",
    "WEIGHT_MAX = 2.0\n",
    "NUM_EPISODES_SNN = 3000\n",
    "\n",
    "patterns_dict, patterns_labels_dict = define_patterns(NUM_INPUTS, BASE_RATE, HIGH_RATE)\n",
    "\n",
    "rl_params = {'alpha': 0.1, 'gamma': 0.9, 'epsilon': 1.0, 'epsilon_decay': 0.99, 'epsilon_min': 0.05}\n",
    "\n",
    "print(\"\\n===== Testing with num_outputs = 4 =====\")\n",
    "snn_net_4 = setup_snn(NUM_INPUTS, 4, CONN_PROB, WEIGHT_MIN, WEIGHT_MAX)\n",
    "env_snn_4 = SNNPatternEnv(snn_net_4, patterns_dict, patterns_labels_dict, PATTERN_DURATION)\n",
    "train_snn_agent(env_snn_4, rl_params, num_episodes=NUM_EPISODES_SNN)\n",
    "\n",
    "print(\"\\n===== Testing with num_outputs = 2 =====\")\n",
    "snn_net_2 = setup_snn(NUM_INPUTS, 2, CONN_PROB, WEIGHT_MIN, WEIGHT_MAX)\n",
    "env_snn_2 = SNNPatternEnv(snn_net_2, patterns_dict, patterns_labels_dict, PATTERN_DURATION)\n",
    "train_snn_agent(env_snn_2, rl_params, num_episodes=NUM_EPISODES_SNN)\n",
    "\n",
    "print(\"\\n===== Testing with num_outputs = 8 =====\")\n",
    "snn_net_8 = setup_snn(NUM_INPUTS, 8, CONN_PROB, WEIGHT_MIN, WEIGHT_MAX)\n",
    "env_snn_8 = SNNPatternEnv(snn_net_8, patterns_dict, patterns_labels_dict, PATTERN_DURATION)\n",
    "train_snn_agent(env_snn_8, rl_params, num_episodes=NUM_EPISODES_SNN)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

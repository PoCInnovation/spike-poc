{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuromorphic Computing meets Reinforcement Learning: A Hands-On Workshop\n",
    "\n",
    "**Welcome!** This workshop will guide you through the fundamentals of neuromorphic computing (NC) and reinforcement learning (RL), culminating in a project that combines both fields. We'll focus on intuitive explanations, mathematical foundations, and practical coding examples.\n",
    "\n",
    "**Target Audience:** Beginners with some Python programming experience. No prior knowledge of NC or RL is strictly required.\n",
    "\n",
    "**Estimated Duration:** 6-8 hours (including breaks and exercises)\n",
    "\n",
    "**Learning Objectives:**\n",
    "* Understand the biological inspiration and core concepts of Neuromorphic Computing.\n",
    "* Simulate basic Spiking Neuron Models (like Leaky Integrate-and-Fire).\n",
    "* Build and simulate simple Spiking Neural Networks (SNNs).\n",
    "* Grasp the fundamentals of Reinforcement Learning (Agents, Environments, Rewards, Policies).\n",
    "* Implement a basic RL algorithm (Q-Learning).\n",
    "* Explore how SNNs can be integrated with RL for potential benefits like energy efficiency.\n",
    "* Implement a simple project combining SNNs and RL.\n",
    "\n",
    "**Libraries We'll Use:**\n",
    "*   **Brian2:** A powerful Python simulator for spiking neural networks.\n",
    "*   **NumPy:** For numerical operations.\n",
    "*   **Matplotlib:** For plotting and visualization.\n",
    "*   **(Optional) Gym/Gymnasium:** For standard RL environments (though we might use custom simple ones)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Setup and Introduction (15 mins)\n",
    "\n",
    "Let's ensure you have the necessary libraries installed and briefly set the stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Installation\n",
    "\n",
    "If you haven't already, please install the required libraries. You can do this by running the following cell (uncomment the lines):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install brian2 numpy matplotlib notebook ipywidgets\n",
    "# Optional for later RL parts, or if using standard environments:\n",
    "# !pip install gymnasium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Importing Libraries\n",
    "We'll import the necessary libraries as we need them, but let's start with the basics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brian2 as b2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time  # For timing simulations\n",
    "from random import sample # Needed for Exercise 2 option\n",
    "\n",
    "# Configure Matplotlib for inline display in Jupyter\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' # For higher resolution plots\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "try:\n",
    "    print(f\"Brian2 version: {b2.__version__}\")\n",
    "    print(f\"NumPy version: {np.__version__}\")\n",
    "except NameError:\n",
    "    print(\"One or more libraries might not be installed correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Workshop Overview\n",
    "\n",
    "*   **Module 1: Neuromorphic Computing Fundamentals:** Biological inspiration, SNNs, LIF neuron model. (≈ 1.5 hours)\n",
    "*   **Module 2: Building Simple Spiking Networks:** Connecting neurons, encoding information, simulation. (≈ 1.5 hours)\n",
    "*   **Module 3: Reinforcement Learning Fundamentals:** The RL loop, Q-Learning. (≈ 1.5 hours)\n",
    "*   **Module 4: Bridging NC and RL:** Why combine them? Simple integrated project. (≈ 1.5 hours)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 1: Neuromorphic Computing Fundamentals (≈ 1.5 hours)\n",
    "\n",
    "Let's dive into the world inspired by the brain!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is Neuromorphic Computing?\n",
    "\n",
    "*   **Biological Inspiration:** Mimicking the structure and function of the biological nervous system (neurons, synapses).\n",
    "*   **Key Differences from Traditional Computing (von Neumann):**\n",
    "    *   *Colocated Memory and Processing:* Reduces the bottleneck of data movement.\n",
    "    *   *Event-Driven Computation:* Computations happen only when \"spikes\" (events) occur, leading to potential energy savings.\n",
    "    *   *Massive Parallelism:* Similar to the brain's architecture.\n",
    "*   **Why Neuromorphic? Potential Advantages:**\n",
    "    *   **Energy Efficiency:** Especially for tasks involving sparse, real-time data.\n",
    "    *   **Real-time Processing:** Handling sensory data streams naturally.\n",
    "    *   **Robustness:** Potential for fault tolerance.\n",
    "    *   **Online Learning:** Adapting continuously, like biological systems.\n",
    "*   **Spiking Neural Networks (SNNs):** The primary computational model in neuromorphic computing. Unlike traditional Artificial Neural Networks (ANNs) that pass continuous values, SNNs communicate using discrete events (spikes) occurring at specific points in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. The Neuron: Biological vs. Artificial\n",
    "\n",
    "*   **Biological Neuron:** Receives signals through dendrites, integrates them at the soma (cell body), and if a threshold is reached, fires an action potential (spike) down the axon.\n",
    "*   **Spiking Neuron Models:** Mathematical abstractions capturing this behavior. Many models exist (Hodgkin-Huxley, Izhikevich, LIF). We'll focus on the **Leaky Integrate-and-Fire (LIF)** model due to its simplicity and computational efficiency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. The Leaky Integrate-and-Fire (LIF) Model\n",
    "\n",
    "Imagine a bucket (membrane potential `V`) with a small leak (leak conductance). Water (current `I`) flows into the bucket. If the water level reaches a certain height (threshold `V_th`), the bucket is instantly emptied (reset `V_reset`) and a signal (spike) is sent. The leak ensures that without continuous input, the water level gradually drops back to a resting level (`V_rest`).\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "\n",
    "The change in membrane potential `V` over time `t` is described by the differential equation:\n",
    "\n",
    "$$ \\tau_m \\frac{dV}{dt} = -(V - V_{rest}) + R_m \\cdot I $$\n",
    "\n",
    "Where:\n",
    "*   `τ_m` (tau_m): Membrane time constant (how quickly the potential changes/leaks). `τ_m = R_m * C_m`\n",
    "*   `V`: Membrane potential.\n",
    "*   `V_rest`: Resting potential.\n",
    "*   `R_m`: Membrane resistance.\n",
    "*   `I`: Input current.\n",
    "\n",
    "**Spiking Mechanism:**\n",
    "*   If `V >= V_th` (threshold potential):\n",
    "    1.  A spike is generated.\n",
    "    2.  `V` is reset to `V_reset`.\n",
    "    3.  (Optional) Refractory period: The neuron cannot spike for a short duration after firing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Simulating a Single LIF Neuron (Brian2)\n",
    "\n",
    "Let's simulate one LIF neuron receiving a constant input current."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.start_scope() # Ensure a clean Brian2 environment\n",
    "\n",
    "# Define LIF parameters\n",
    "tau_m = 10 * b2.ms       # Membrane time constant\n",
    "V_rest = -65 * b2.mV     # Resting potential\n",
    "V_reset = -65 * b2.mV    # Reset potential\n",
    "V_th = -50 * b2.mV       # Firing threshold\n",
    "R_m = 100 * b2.Mohm      # Membrane resistance (using explicit Rm*I)\n",
    "refractory_period_base = 5 * b2.ms # Base refractory period\n",
    "\n",
    "# Define the LIF neuron equations\n",
    "# Note: Brian2 uses string-based equations\n",
    "lif_eqs = '''\n",
    "dv/dt = (-(v - V_rest) + R_m * I) / tau_m : volt (unless refractory)\n",
    "I : amp # Input current - defined externally\n",
    "'''\n",
    "\n",
    "# Create a NeuronGroup (1 neuron)\n",
    "# threshold='v > V_th': condition for firing\n",
    "# reset='v = V_reset': action taken after firing\n",
    "# refractory=refractory_period_base: period after firing where neuron cannot fire again\n",
    "# method='exact': numerical integration method (exact for linear LIF)\n",
    "G = b2.NeuronGroup(1, lif_eqs, threshold='v > V_th', reset='v = V_reset',\n",
    "                   refractory=refractory_period_base, method='exact')\n",
    "\n",
    "# Initialize membrane potential\n",
    "G.v = V_rest\n",
    "\n",
    "# Provide a constant input current\n",
    "input_current = 200 * b2.pA # Picoamperes\n",
    "G.I = input_current\n",
    "\n",
    "# Set up Monitors to record data\n",
    "# SpikeMonitor records spike times and indices of firing neurons\n",
    "spike_monitor = b2.SpikeMonitor(G)\n",
    "# StateMonitor records the evolution of variables (like 'v') over time\n",
    "state_monitor = b2.StateMonitor(G, 'v', record=0) # record=0 means record for neuron index 0\n",
    "\n",
    "# --- Run the Simulation ---\n",
    "simulation_duration = 100 * b2.ms\n",
    "b2.run(simulation_duration)\n",
    "\n",
    "# --- Plot the Results ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot Membrane Potential\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(state_monitor.t / b2.ms, state_monitor.v[0] / b2.mV, label='Membrane Potential')\n",
    "plt.axhline(V_th / b2.mV, color='red', linestyle='--', label='Threshold V_th')\n",
    "plt.axhline(V_rest / b2.mV, color='gray', linestyle=':', label='Resting V_rest')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Potential (mV)')\n",
    "plt.title(f'LIF Neuron Response to Constant Current ({input_current})')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Spikes (Raster Plot)\n",
    "plt.subplot(2, 1, 2)\n",
    "if spike_monitor.num_spikes > 0:\n",
    "    plt.plot(spike_monitor.t / b2.ms, spike_monitor.i, '.k', label='Spikes') # '.k' means black dots\n",
    "else:\n",
    "    plt.plot([], [], '.k', label='Spikes') # Plot empty if no spikes\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Neuron Index')\n",
    "plt.yticks([]) # Only one neuron, so hide y-axis ticks\n",
    "plt.title('Spike Output')\n",
    "plt.grid(True)\n",
    "plt.ylim(-0.5, 0.5) # Adjust y-limits for single neuron visibility\n",
    "plt.xlim(0, simulation_duration / b2.ms)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Number of spikes: {spike_monitor.num_spikes}\")\n",
    "if spike_monitor.num_spikes > 0:\n",
    "    print(f\"Spike times: {spike_monitor.t / b2.ms} ms\")\n",
    "else:\n",
    "    print(\"No spikes occurred.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Exercise 1: Explore Neuron Behavior (20 mins)\n",
    "\n",
    "1.  **Modify the Input Current:** Rerun the simulation above with different values for `input_current` (e.g., `100 * b2.pA`, `150 * b2.pA`, `300 * b2.pA`). How does the firing rate (number of spikes per second) change?\n",
    "2.  **Change the Time Constant:** Reset the current to `200 * b2.pA`. Now, change `tau_m` (e.g., to `5 * b2.ms` or `20 * b2.ms`). How does this affect how quickly the neuron reaches threshold and the resulting firing pattern?\n",
    "3.  **Introduce Refractoriness:** If you set `refractory=0*b2.ms`, what happens? Compare it to a longer refractory period like `10*b2.ms`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- Exercise 1 Code Space ---\n",
    "\n",
    "# Remember to clear the default Brian2 network if running multiple times:\n",
    "b2.start_scope()\n",
    "\n",
    "# --- Your Code Here ---\n",
    "\n",
    "# 1. Define Neuron Parameters\n",
    "\n",
    "# 2. Define Neuron Equations (usually same as before for LIF)\n",
    "\n",
    "# 3. Create the NeuronGroup\n",
    "\n",
    "# 4. Initialize Neuron State\n",
    "\n",
    "# 5. Set up Monitors\n",
    "\n",
    "# 6. Run the Simulation\n",
    "\n",
    "# 7. Plot the Results (copy or adapt the plotting code from section 1.4)\n",
    "\n",
    "# 8. Print relevant information (spike count, firing rate)\n",
    "\n",
    "print(\"\\nExercise complete. Reflect on the changes observed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6. Synapses and Basic Plasticity (Brief Overview)\n",
    "\n",
    "*   **Synapses:** Connections between neurons. When a presynaptic neuron spikes, it typically causes a change (increase or decrease) in the postsynaptic neuron's membrane potential after some delay. This change is often modeled as a brief pulse of current or a change in conductance.\n",
    "*   **Synaptic Weight (`w`):** Represents the strength of a connection. A positive weight is excitatory (increases V_post), a negative weight is inhibitory (decreases V_post).\n",
    "*   **Synaptic Plasticity:** The ability of synaptic weights to change over time based on neural activity. This is the basis of learning and memory in the brain.\n",
    "    *   **Spike-Timing-Dependent Plasticity (STDP):** A common biologically observed rule where the change in synaptic weight depends on the *relative timing* of pre- and postsynaptic spikes.\n",
    "        *   If pre spikes just *before* post -> Strengthen synapse (Potentiation, LTP).\n",
    "        *   If pre spikes just *after* post -> Weaken synapse (Depression, LTD).\n",
    "\n",
    "(We won't implement STDP in detail now, but it's a key concept in neuromorphic learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 2: Building Simple Spiking Networks (≈ 1.5 hours)\n",
    "\n",
    "Now let's connect multiple neurons and see how they interact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Connecting Neurons: Synapses in Brian2\n",
    "\n",
    "Brian2's `Synapses` object connects `NeuronGroup`s. We need to define:\n",
    "*   The presynaptic group (`source`).\n",
    "*   The postsynaptic group (`target`).\n",
    "*   The *model* of the synapse (what happens on a spike).\n",
    "*   The connection pattern (`connect()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Encoding Information: From Data to Spikes\n",
    "\n",
    "Since SNNs process spikes, we need ways to convert real-world data into spike trains:\n",
    "\n",
    "*   **Rate Coding:** The *frequency* of spikes represents the intensity of a stimulus. Higher intensity = higher firing rate. Simple but potentially slow.\n",
    "*   **Temporal Coding:** The precise *timing* of individual spikes carries information. Potentially much faster and more efficient.\n",
    "*   **Population Coding:** Information is encoded in the *pattern* of activity across a group of neurons.\n",
    "\n",
    "For simplicity, we'll often use **Poisson Spike Trains:** Spikes occur randomly with a specific average rate. A `PoissonGroup` in Brian2 generates these easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Example: A Simple Feedforward Network\n",
    "\n",
    "Let's build a network: Input Layer -> Output Layer.\n",
    "*   Input Layer: Generates Poisson spikes.\n",
    "*   Output Layer: LIF neurons receiving spikes from the input layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2.start_scope() # Clear previous Brian2 objects\n",
    "\n",
    "# --- Parameters ---\n",
    "num_inputs = 50\n",
    "num_outputs = 10\n",
    "input_rate = 20 * b2.Hz  # Average firing rate for input neurons\n",
    "simulation_duration = 200 * b2.ms\n",
    "\n",
    "# Output LIF neuron parameters (same as before)\n",
    "tau_m = 10 * b2.ms\n",
    "V_rest = -65 * b2.mV\n",
    "V_reset = -65 * b2.mV\n",
    "V_th = -50 * b2.mV\n",
    "lif_eqs = '''\n",
    "dv/dt = -(v - V_rest) / tau_m : volt (unless refractory)\n",
    "''' # Removed Rm*I, current comes from synapses\n",
    "\n",
    "# --- Network Components ---\n",
    "# Input Layer: Poisson neurons\n",
    "input_group = b2.PoissonGroup(num_inputs, rates=input_rate)\n",
    "\n",
    "# Output Layer: LIF neurons\n",
    "output_group = b2.NeuronGroup(num_outputs, lif_eqs, threshold='v > V_th',\n",
    "                              reset='v = V_reset', refractory=5*b2.ms, method='exact')\n",
    "output_group.v = V_rest # Initialize potential\n",
    "\n",
    "# --- Synapses: Connecting Input to Output ---\n",
    "# Define synaptic model: On a presynaptic spike, increase postsynaptic 'v' by 'w'\n",
    "# 'w' represents the synaptic weight\n",
    "synapse_model = 'w : volt' # Define 'w' as a synaptic variable (units of voltage change)\n",
    "on_pre_eq = 'v_post += w' # Action executed when a presynaptic spike arrives\n",
    "\n",
    "# Create synapses\n",
    "synapses = b2.Synapses(input_group, output_group, model=synapse_model, on_pre=on_pre_eq)\n",
    "\n",
    "# Connect all input neurons to all output neurons (full connectivity)\n",
    "synapses.connect()\n",
    "\n",
    "# Set synaptic weights (e.g., excitatory, randomly distributed)\n",
    "# Let's make the weights cause a small depolarization\n",
    "min_weight_val = 0.5 # value in mV\n",
    "max_weight_val = 1.5 # value in mV\n",
    "synapses.w = np.random.uniform(min_weight_val, max_weight_val, size=len(synapses)) * b2.mV\n",
    "\n",
    "# --- Monitors ---\n",
    "input_spike_mon = b2.SpikeMonitor(input_group, name='InputSpikes')\n",
    "output_spike_mon = b2.SpikeMonitor(output_group, name='OutputSpikes')\n",
    "output_state_mon = b2.StateMonitor(output_group, 'v', record=range(num_outputs), name='OutputState') # Record potential for all output neurons\n",
    "\n",
    "# --- Run Simulation ---\n",
    "print(f\"Running simulation for {simulation_duration}...\")\n",
    "b2.run(simulation_duration)\n",
    "print(\"Simulation complete.\")\n",
    "\n",
    "# --- Visualization: Raster Plots ---\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Input Spikes\n",
    "plt.subplot(2, 1, 1)\n",
    "if input_spike_mon.num_spikes > 0:\n",
    "    plt.plot(input_spike_mon.t / b2.ms, input_spike_mon.i, '.k', markersize=2)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Input Neuron Index')\n",
    "plt.title(f'Input Layer Spikes ({num_inputs} Poisson Neurons @ {input_rate})')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, simulation_duration / b2.ms)\n",
    "plt.ylim(-1, num_inputs)\n",
    "\n",
    "# Output Spikes\n",
    "plt.subplot(2, 1, 2)\n",
    "if output_spike_mon.num_spikes > 0:\n",
    "    plt.plot(output_spike_mon.t / b2.ms, output_spike_mon.i, '.r', markersize=4)\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Output Neuron Index')\n",
    "plt.title(f'Output Layer Spikes ({num_outputs} LIF Neurons)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(0, simulation_duration / b2.ms)\n",
    "plt.ylim(-1, num_outputs)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization: Membrane Potential of Output Neurons ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "for i in range(num_outputs): # Plot potential for all output neurons\n",
    "    plt.plot(output_state_mon.t / b2.ms, output_state_mon.v[i] / b2.mV, label=f'Neuron {i}')\n",
    "\n",
    "plt.axhline(V_th / b2.mV, color='red', linestyle='--', label='Threshold V_th')\n",
    "plt.xlabel('Time (ms)')\n",
    "plt.ylabel('Potential (mV)')\n",
    "plt.title('Membrane Potential of Output Neurons')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total input spikes: {input_spike_mon.num_spikes}\")\n",
    "print(f\"Total output spikes: {output_spike_mon.num_spikes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Exercise 2: Network Dynamics (25 mins)\n",
    "\n",
    "1.  **Synaptic Strength:** What happens if you significantly increase the average synaptic weight `w` (e.g., make `min_weight_val = 2.0`, `max_weight_val = 3.0`)? What if you make the weights inhibitory (negative)? Modify the code above to test this.\n",
    "2.  **Connectivity:** Instead of `synapses.connect()`, try connecting neurons sparsely. For example, connect each input neuron to only 5 random output neurons. You can use `synapses.connect(j='k for k in sample(range(num_outputs), size=5)')` or a probability `synapses.connect(p=0.1)`. How does this affect the output activity?\n",
    "3.  **Input Rate:** Change `input_rate`. How does the output firing rate respond? Is the relationship linear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 2 Code Space ---\n",
    "b2.start_scope()\n",
    "\n",
    "# --- Your Code Here ---\n",
    "\n",
    "# 1. Define Network Parameters\n",
    "\n",
    "# 2. Define Neuron Parameters (e.g., LIF, usually same as before)\n",
    "\n",
    "# 3. Create Neuron Groups\n",
    "\n",
    "# 4. Create Synapses Object\n",
    "\n",
    "# 5. Define Connectivity\n",
    "\n",
    "# 6. Set Synaptic Weights\n",
    "\n",
    "# 7. Set up Monitors\n",
    "\n",
    "# 8. Run the Simulation\n",
    "\n",
    "# 9. Plot the Results (copy or adapt plotting code from section 2.3)\n",
    "\n",
    "# 10. Print relevant info\n",
    "\n",
    "print(\"\\nExercise complete. Reflect on how network structure and parameters influence activity.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5. Neuromorphic Hardware (Brief Mention)\n",
    "\n",
    "*   Specialized hardware designed to run SNNs efficiently.\n",
    "*   Examples: Intel Loihi/Loihi 2, SpiNNaker (Manchester University), TrueNorth (IBM - older), BrainScaleS (Heidelberg University), Akida (BrainChip), DynapSE (iniVation).\n",
    "*   Often feature asynchronous, event-driven processing and low power consumption.\n",
    "*   Simulators like Brian2 are essential for developing algorithms before deploying to hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 3: Reinforcement Learning Fundamentals (≈ 1.5 hours)\n",
    "\n",
    "Shifting gears to learning from interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. What is Reinforcement Learning?\n",
    "\n",
    "*   RL is a type of machine learning where an **agent** learns to make decisions by interacting with an **environment**.\n",
    "*   The agent performs **actions**, receives **observations** (about the state of the environment), and gets **rewards** (or penalties).\n",
    "*   The goal of the agent is to learn a **policy** (a strategy for choosing actions) that maximizes its cumulative reward over time.\n",
    "\n",
    "**The Agent-Environment Loop:**\n",
    "\n",
    "1.  Agent observes the current **state** (`s_t`).\n",
    "2.  Agent chooses an **action** (`a_t`) based on its policy.\n",
    "3.  Environment transitions to a new **state** (`s_{t+1}`) based on (`s_t`, `a_t`).\n",
    "4.  Environment provides a **reward** (`r_{t+1}`) to the agent.\n",
    "5.  Repeat.\n",
    "\n",
    "*(Diagram: A simple loop showing Agent -> Action -> Environment -> State/Reward -> Agent)*\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Agent] -- Action (a_t) --> E[Environment];\n",
    "    E -- State (s_{t+1}), Reward (r_{t+1}) --> A;\n",
    "```\n",
    "(Requires mermaid rendering support in your Jupyter environment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Key Concepts\n",
    "\n",
    "*   **Agent:** The learner and decision-maker.\n",
    "*   **Environment:** Everything outside the agent that it interacts with.\n",
    "*   **State (`s`):** A representation of the environment's current situation.\n",
    "*   **Action (`a`):** A choice the agent can make.\n",
    "*   **Reward (`r`):** A scalar feedback signal indicating how good the last action was in that state.\n",
    "*   **Policy (`π(a|s)`):** The agent's strategy; defines the probability of taking action `a` in state `s`.\n",
    "*   **Value Function:** Predicts the expected future reward.\n",
    "    *   **State-Value Function (`V(s)`):** Expected cumulative reward starting from state `s` and following policy `π`.\n",
    "    *   **Action-Value Function (`Q(s, a)`):** Expected cumulative reward starting from state `s`, taking action `a`, and then following policy `π`. This is often more useful for choosing actions.\n",
    "*   **Discount Factor (`γ`, gamma):** A value between 0 and 1 that determines the importance of future rewards. Rewards received sooner are often valued more than rewards received later. `γ=0` means only immediate reward matters, `γ≈1` means future rewards are highly valued.\n",
    "*   **Markov Decision Process (MDP):** The mathematical framework for RL problems assuming the \"Markov property\" (the current state fully captures all necessary information from the past). Defined by (S, A, P, R, γ): States, Actions, Transition Probabilities `P(s'|s, a)`, Reward Function `R(s, a, s')`, Discount Factor `γ`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Q-Learning: A Simple RL Algorithm\n",
    "\n",
    "*   Q-Learning is a **model-free**, **off-policy** RL algorithm.\n",
    "    *   *Model-free:* It doesn't need to know the environment's transition probabilities (`P`) or reward function (`R`). It learns directly from experience.\n",
    "    *   *Off-policy:* It learns the optimal Q-values regardless of the policy being followed during exploration (e.g., epsilon-greedy).\n",
    "*   **Goal:** Learn the optimal action-value function `Q*(s, a)`.\n",
    "*   **Q-Table:** In simple problems with discrete states and actions, we can store the Q-values in a table (e.g., a dictionary or array) where rows are states and columns are actions.\n",
    "\n",
    "**The Q-Learning Update Rule:**\n",
    "\n",
    "When the agent takes action `a_t` in state `s_t`, observes reward `r_{t+1}` and next state `s_{t+1}`, it updates the Q-value using:\n",
    "\n",
    "$$ Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha \\left[ r_{t+1} + \\gamma \\max_{a'} Q(s_{t+1}, a') - Q(s_t, a_t) \\right] $$\n",
    "\n",
    "Where:\n",
    "*   `α` (alpha): Learning rate (0 < α ≤ 1), controls how much new information overrides old information.\n",
    "*   `γ` (gamma): Discount factor (0 ≤ γ ≤ 1).\n",
    "*   `max_{a'} Q(s_{t+1}, a')`: The maximum Q-value for the *next* state `s_{t+1}` over all possible next actions `a'`. This represents the agent's current estimate of the best possible future value from `s_{t+1}`.\n",
    "*   The term in the square brackets `[...]` is the **Temporal Difference (TD) error**: the difference between the estimated return (`r + γ * max Q`) and the current Q-value.\n",
    "\n",
    "**Exploration vs. Exploitation:**\n",
    "*   To find the optimal policy, the agent needs to explore different actions.\n",
    "*   But it also needs to exploit its current knowledge to get rewards.\n",
    "*   **Epsilon-Greedy (`ε`-greedy):** A common strategy:\n",
    "    *   With probability `ε` (epsilon), choose a random action (explore).\n",
    "    *   With probability `1-ε`, choose the action with the highest Q-value for the current state (exploit).\n",
    "    *   `ε` often starts high (e.g., 1.0) and decays over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Implementing Tabular Q-Learning: Simple Grid World Example\n",
    "\n",
    "Let's create a simple text-based grid world environment and apply Q-Learning.\n",
    "\n",
    "**Environment:**\n",
    "*   A 4x4 grid.\n",
    "*   Agent starts at (0, 0).\n",
    "*   Goal is at (3, 3) (reward +10).\n",
    "*   A \"hole\" is at (1, 1) (reward -10).\n",
    "*   Moving into a wall keeps the agent in place.\n",
    "*   Small negative reward (-0.1) for each step to encourage efficiency.\n",
    "*   Actions: Up, Down, Left, Right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Grid World Environment\n",
    "class GridWorldEnv:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.agent_pos = (0, 0)\n",
    "        self.goal_pos = (size - 1, size - 1)\n",
    "        self.hole_pos = (1, 1)\n",
    "        # Actions: 0: Up, 1: Down, 2: Left, 3: Right\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "        self.action_delta = {\n",
    "            0: (-1, 0), # Up\n",
    "            1: (1, 0),  # Down\n",
    "            2: (0, -1), # Left\n",
    "            3: (0, 1)   # Right\n",
    "        }\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_pos = (0, 0)\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        # Represent state as a unique integer or tuple\n",
    "        return self.agent_pos\n",
    "\n",
    "    def step(self, action):\n",
    "        if action not in self.actions:\n",
    "            raise ValueError(\"Invalid action\")\n",
    "\n",
    "        delta = self.action_delta[action]\n",
    "        current_r, current_c = self.agent_pos\n",
    "        next_r, next_c = current_r + delta[0], current_c + delta[1]\n",
    "\n",
    "        # Check boundaries\n",
    "        if not (0 <= next_r < self.size and 0 <= next_c < self.size):\n",
    "            next_r, next_c = current_r, current_c # Stay in place if wall hit\n",
    "\n",
    "        self.agent_pos = (next_r, next_c)\n",
    "        next_state = self.get_state()\n",
    "\n",
    "        # Determine reward and done status\n",
    "        if self.agent_pos == self.goal_pos:\n",
    "            reward = 10.0\n",
    "            done = True\n",
    "        elif self.agent_pos == self.hole_pos:\n",
    "            reward = -10.0\n",
    "            done = True\n",
    "        else:\n",
    "            reward = -0.1 # Step penalty\n",
    "            done = False\n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "    def render(self):\n",
    "        grid = np.full((self.size, self.size), '_', dtype=str)\n",
    "        grid[self.goal_pos] = 'G'\n",
    "        grid[self.hole_pos] = 'H'\n",
    "        grid[self.agent_pos] = 'A'\n",
    "        print(\"\\n\".join(\" \".join(row) for row in grid))\n",
    "        print(\"-\" * (self.size * 2 - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Learning Agent\n",
    "class QLearningAgent:\n",
    "    def __init__(self, env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01):\n",
    "        self.env = env\n",
    "        self.q_table = {} # Using dict: {(state): {action: q_value}}\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        # Check if env provides actions, otherwise assume standard [0, 1, 2, 3]\n",
    "        self.actions = getattr(env, 'actions', [0, 1, 2, 3]) \n",
    "\n",
    "    def get_q_value(self, state, action):\n",
    "        # Return Q-value, default to 0 if state or action not seen\n",
    "        return self.q_table.get(state, {}).get(action, 0.0)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Explore: choose random action\n",
    "            return np.random.choice(self.actions)\n",
    "        else:\n",
    "            # Exploit: choose best action based on Q-values\n",
    "            q_values = [self.get_q_value(state, a) for a in self.actions]\n",
    "            # Handle cases where state hasn't been fully explored or all Q-values are 0\n",
    "            max_q = np.max(q_values)\n",
    "            # Check if all Q-values are the same (e.g., all zero for a new state)\n",
    "            if all(q == q_values[0] for q in q_values):\n",
    "                # If all are same, choose randomly among all actions\n",
    "                 best_actions = self.actions\n",
    "            else:\n",
    "                # Choose among actions with the max Q-value\n",
    "                 best_actions = [a for a, q in zip(self.actions, q_values) if q == max_q]\n",
    "            return np.random.choice(best_actions)\n",
    "\n",
    "    def update_q_table(self, state, action, reward, next_state, done):\n",
    "        # Q-learning update rule\n",
    "        old_q = self.get_q_value(state, action)\n",
    "        \n",
    "        # If the episode is done, the future reward estimate (from next_state) is 0\n",
    "        if done:\n",
    "            td_target = reward \n",
    "        else:\n",
    "            next_max_q = np.max([self.get_q_value(next_state, a) for a in self.actions])\n",
    "            td_target = reward + self.gamma * next_max_q\n",
    "            \n",
    "        td_error = td_target - old_q\n",
    "        new_q = old_q + self.alpha * td_error\n",
    "\n",
    "        # Update the table\n",
    "        if state not in self.q_table:\n",
    "            self.q_table[state] = {act: 0.0 for act in self.actions}\n",
    "        self.q_table[state][action] = new_q\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Training Loop ---\n",
    "env = GridWorldEnv(size=4)\n",
    "agent = QLearningAgent(env, alpha=0.1, gamma=0.99, epsilon=1.0, epsilon_decay=0.995, epsilon_min=0.01)\n",
    "\n",
    "num_episodes = 5000\n",
    "max_steps_per_episode = 100\n",
    "rewards_per_episode = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        agent.update_q_table(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    agent.update_epsilon() # Decay epsilon after each episode\n",
    "    rewards_per_episode.append(total_reward)\n",
    "\n",
    "    if (episode + 1) % (num_episodes // 10) == 0:\n",
    "        # Calculate average reward over the last N episodes for smoother reporting\n",
    "        avg_reward = np.mean(rewards_per_episode[-(num_episodes // 10):])\n",
    "        print(f\"Episode {episode + 1}/{num_episodes} | Avg Reward (last {num_episodes // 10}): {avg_reward:.2f} | Epsilon: {agent.epsilon:.3f}\")\n",
    "\n",
    "end_time = time.time()\n",
    "print(f\"\\nTraining finished in {end_time - start_time:.2f} seconds.\")\n",
    "\n",
    "# --- Plot Training Progress ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "# Smooth rewards for better visualization (moving average)\n",
    "window_size = 100\n",
    "if len(rewards_per_episode) >= window_size:\n",
    "    smoothed_rewards = np.convolve(rewards_per_episode, np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(smoothed_rewards)\n",
    "    plt.xlabel(f'Episode (Moving Average over {window_size} episodes)')\n",
    "else:\n",
    "    plt.plot(rewards_per_episode) # Plot raw rewards if not enough episodes for smoothing\n",
    "    plt.xlabel('Episode')\n",
    "    \n",
    "plt.title('Episode Rewards over Time')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Display Learned Policy (Optional Visualization) ---\n",
    "print(\"\\nLearned Policy (Arrows indicate best action):\")\n",
    "action_arrows = {0: '^', 1: 'v', 2: '<', 3: '>'}\n",
    "policy_grid = np.full((env.size, env.size), ' ', dtype=str)\n",
    "policy_grid[env.goal_pos] = 'G'\n",
    "policy_grid[env.hole_pos] = 'H'\n",
    "\n",
    "for r in range(env.size):\n",
    "    for c in range(env.size):\n",
    "        state = (r, c)\n",
    "        if state != env.goal_pos and state != env.hole_pos:\n",
    "            if state in agent.q_table:\n",
    "                # Choose best action based on learned Q-values (exploitation)\n",
    "                q_values = [agent.get_q_value(state, a) for a in agent.actions]\n",
    "                best_action = agent.actions[np.argmax(q_values)]\n",
    "                policy_grid[r, c] = action_arrows[best_action]\n",
    "            else:\n",
    "                 policy_grid[r, c] = '.' # State not visited or no clear policy\n",
    "\n",
    "print(\"\\n\".join(\" \".join(row) for row in policy_grid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5. Exercise 3: Tune Q-Learning Parameters (20 mins)\n",
    "\n",
    "1.  **Learning Rate (`alpha`):** Rerun the training with a much smaller `alpha` (e.g., 0.01) and a much larger `alpha` (e.g., 0.9). How does this affect the learning speed and the stability of the final rewards/policy?\n",
    "2.  **Discount Factor (`gamma`):** What happens if `gamma` is very low (e.g., 0.1)? The agent becomes \"myopic\". What if `gamma` is 1.0? (Note: gamma=1 can sometimes cause issues if rewards can accumulate indefinitely without termination). Try `gamma=0.9`.\n",
    "3.  **Epsilon Decay (`epsilon_decay`):** Make `epsilon_decay` very close to 1 (e.g., 0.999) so exploration lasts longer. Make it decay faster (e.g., 0.9). How does the shape of the reward curve change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 3 Code Space ---\n",
    "\n",
    "# --- Your Code Here ---\n",
    "\n",
    "# 1. Define the Hyperparameters to test\n",
    "\n",
    "# 2. Create the Environment (same as before)\n",
    "\n",
    "# 3. Create the QLearningAgent with the modified hyperparameters\n",
    "\n",
    "# 4. Implement the Training Loop (copy or adapt from section 3.4)\n",
    "\n",
    "# 5. Plot the results (copy or adapt from section 3.4)\n",
    "\n",
    "# 6. Display the learned policy (copy or adapt from section 3.4)\n",
    "\n",
    "print(\"\\nExercise complete. Reflect on how hyperparameters influence RL performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module 4: Bridging Neuromorphic Computing and RL (≈ 1.5 hours)\n",
    "\n",
    "Let's bring the two fields together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Why Combine Neuromorphic Computing and RL?\n",
    "\n",
    "*   **Energy Efficiency:** Running RL agents (especially complex ones) can be computationally expensive. Neuromorphic hardware offers the potential for significant power savings, particularly for agents interacting with real-time, sparse sensory data (like robots).\n",
    "*   **Biologically Plausible Learning:** The brain learns through mechanisms that resemble both RL (dopamine signals acting like rewards) and SNNs (spiking activity, synaptic plasticity like STDP). Combining them moves towards more brain-like AI.\n",
    "*   **Event-Based Processing:** SNNs naturally handle asynchronous, event-based inputs, which is common in robotics and sensory processing tasks where RL is applied.\n",
    "*   **Temporal Dynamics:** SNNs inherently process information over time, which could be advantageous for RL tasks requiring memory or sensitivity to timing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Challenges\n",
    "\n",
    "*   **Credit Assignment:** How do you determine which specific synapse or neuron contributed to a reward that might be received much later? This is harder in SNNs with their complex temporal dynamics than in traditional ANNs (solved partially by backpropagation).\n",
    "*   **Learning Rules:** Developing effective, stable, and efficient learning rules for SNNs in an RL context is an active area of research (e.g., Reward-modulated STDP, approximations of backpropagation for SNNs).\n",
    "*   **Simulation Speed:** Simulating large SNNs can be slower than training ANNs on GPUs (though neuromorphic hardware aims to overcome this).\n",
    "*   **Encoding/Decoding:** Converting states and actions between the continuous/discrete world of RL environments and the spiking world of SNNs requires careful design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Approach: SNN as a Function Approximator for RL\n",
    "\n",
    "A common approach (especially for bridging the gap) is to use the SNN to approximate a component of the RL algorithm, similar to how Deep RL uses ANNs:\n",
    "\n",
    "1.  **SNN as Policy Network:** The SNN receives the state (encoded as spikes) and its output firing rates determine the probability of taking each action. Learning adjusts synaptic weights to favor actions leading to higher rewards.\n",
    "2.  **SNN as Value Network:** The SNN receives the state (and potentially action) encoded as spikes, and its output firing rate (or some other measure) represents the estimated Q-value or V-value. Learning adjusts weights to make these estimates more accurate.\n",
    "\n",
    "**Simplification for this Workshop:**\n",
    "Implementing complex SNN learning rules (like reward-modulated STDP) from scratch is time-consuming. We will take a *simpler, hybrid approach*:\n",
    "\n",
    "*   Use an SNN to *process* the input (like a feature extractor).\n",
    "*   Use the *output firing rates* of the SNN as the *state representation* for our existing Tabular Q-Learner.\n",
    "\n",
    "This demonstrates the integration concept without delving into complex SNN-specific RL algorithms immediately. The SNN adds a temporal processing layer before the standard RL decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Project: SNN-Enhanced Agent for a Pattern Recognition Task\n",
    "\n",
    "**Goal:** Train an agent to distinguish between two different input *spike patterns* using RL, where the SNN processes the patterns.\n",
    "\n",
    "**Environment:**\n",
    "*   Generates one of two predefined spike patterns as input.\n",
    "*   Agent must output Action 0 if Pattern A is detected, Action 1 if Pattern B is detected.\n",
    "*   Reward: +1 for correct classification, -1 for incorrect.\n",
    "\n",
    "**SNN Structure:**\n",
    "*   Input Layer: `PoissonGroup` neurons, whose *rates* are modulated according to the pattern being presented.\n",
    "*   Output Layer: A small number of LIF neurons receiving input from the Input Layer.\n",
    "\n",
    "**RL Integration:**\n",
    "*   **State:** The *average firing rates* of the Output Layer neurons over a short time window. This rate vector becomes the `state` for the Q-learner (after discretization).\n",
    "*   **Action:** Chosen by the Q-learner based on the SNN-derived state (Action 0 or 1).\n",
    "*   **Learning:** The Q-learner updates its table based on the (SNN state, chosen action, reward). The SNN weights themselves remain *fixed* in this simplified example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- SNN Setup ---\n",
    "b2.start_scope()\n",
    "\n",
    "# SNN Parameters\n",
    "num_inputs = 20\n",
    "num_outputs = 4 # SNN output neurons (can be tuned in Exercise 4)\n",
    "tau_m = 10 * b2.ms\n",
    "V_rest = -65 * b2.mV\n",
    "V_reset = -65 * b2.mV\n",
    "V_th = -50 * b2.mV\n",
    "lif_eqs = 'dv/dt = -(v - V_rest) / tau_m : volt (unless refractory)'\n",
    "\n",
    "# Input Group (rates will be set dynamically)\n",
    "input_group = b2.PoissonGroup(num_inputs, rates=0*b2.Hz, name='input_layer')\n",
    "\n",
    "# Output Group\n",
    "output_group = b2.NeuronGroup(num_outputs, lif_eqs, threshold='v > V_th',\n",
    "                              reset='v = V_reset', refractory=5*b2.ms, method='exact',\n",
    "                              name='output_layer')\n",
    "output_group.v = V_rest\n",
    "\n",
    "# Synapses (fixed weights for this example)\n",
    "synapses = b2.Synapses(input_group, output_group, 'w : volt', on_pre='v_post += w',\n",
    "                         name='synapses')\n",
    "connection_probability = 0.5 # Can be tuned in Exercise 4\n",
    "synapses.connect(p=connection_probability) \n",
    "min_weight_snn = 0.5 # mV, Can be tuned in Exercise 4\n",
    "max_weight_snn = 2.0 # mV, Can be tuned in Exercise 4\n",
    "if len(synapses) > 0:\n",
    "    synapses.w = np.random.uniform(min_weight_snn, max_weight_snn, size=len(synapses)) * b2.mV \n",
    "\n",
    "# Monitors\n",
    "output_spike_mon = b2.SpikeMonitor(output_group, name='output_spikes')\n",
    "# We need spikes to calculate average rate for the state\n",
    "\n",
    "# Store the network components for easy access\n",
    "# Important: Use include_scope=False if Brian objects defined outside the Network call\n",
    "# Collect all Brian2 objects in the current scope into a Network object\n",
    "snn_network = b2.Network(b2.collect()) \n",
    "snn_network.store('initial_snn') # Store initial state (topology, parameters)\n",
    "print(\"SNN components defined.\")\n",
    "print(f\"Input: {input_group.N}, Output: {output_group.N}, Synapses: {len(synapses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Spike Patterns ---\n",
    "pattern_duration = 100 * b2.ms\n",
    "base_rate_val = 10 # Hz value (can be tuned in Exercise 4)\n",
    "high_rate_val = 50 # Hz value (can be tuned in Exercise 4)\n",
    "\n",
    "# Pattern A: First half of inputs fire at high rate\n",
    "pattern_A_rates = np.zeros(num_inputs) * b2.Hz\n",
    "pattern_A_rates[:num_inputs // 2] = high_rate_val * b2.Hz\n",
    "pattern_A_rates[num_inputs // 2:] = base_rate_val * b2.Hz\n",
    "\n",
    "# Pattern B: Second half of inputs fire at high rate\n",
    "pattern_B_rates = np.zeros(num_inputs) * b2.Hz\n",
    "pattern_B_rates[:num_inputs // 2] = base_rate_val * b2.Hz\n",
    "pattern_B_rates[num_inputs // 2:] = high_rate_val * b2.Hz\n",
    "\n",
    "patterns = {'A': pattern_A_rates, 'B': pattern_B_rates}\n",
    "pattern_labels = {'A': 0, 'B': 1} # Target actions for RL\n",
    "\n",
    "print(\"Spike patterns defined.\")\n",
    "print(f\"Pattern A rates (first 5): {pattern_A_rates[:5]}\")\n",
    "print(f\"Pattern B rates (last 5): {pattern_B_rates[-5:]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- RL Environment using the SNN ---\n",
    "class SNNPatternEnv:\n",
    "    def __init__(self, snn_network, patterns, pattern_labels, duration):\n",
    "        self.snn_network = snn_network\n",
    "        self.patterns = patterns\n",
    "        self.pattern_labels = pattern_labels\n",
    "        self.duration = duration\n",
    "        self.current_pattern_name = None\n",
    "        # RL specifics\n",
    "        self.actions = [0, 1] # Action 0 for Pattern A, Action 1 for Pattern B\n",
    "\n",
    "    def reset(self):\n",
    "        # Choose a random pattern to present\n",
    "        self.current_pattern_name = np.random.choice(list(self.patterns.keys()))\n",
    "        input_rates = self.patterns[self.current_pattern_name]\n",
    "\n",
    "        # Restore the SNN to its initial state and set input rates\n",
    "        self.snn_network.restore('initial_snn') # Reset neuron states, time etc.\n",
    "        # Access PoissonGroup by name ('input_layer') defined during creation\n",
    "        self.snn_network['input_layer'].rates = input_rates \n",
    "\n",
    "        # Run SNN for the specified duration to get initial state\n",
    "        self.snn_network.run(self.duration, report='off') \n",
    "\n",
    "        # Get the state representation (discretized average firing rates)\n",
    "        state = self._get_snn_state()\n",
    "        return state\n",
    "\n",
    "    def _get_snn_state(self):\n",
    "        # Calculate average firing rate over the duration using SpikeMonitor\n",
    "        spike_monitor = self.snn_network['output_spikes']\n",
    "        num_output_neurons = self.snn_network['output_layer'].N\n",
    "        rates = np.zeros(num_output_neurons)\n",
    "        duration_sec = self.duration / b2.second\n",
    "        \n",
    "        if duration_sec > 0 and spike_monitor.num_spikes > 0:\n",
    "            # Count spikes per neuron\n",
    "            neuron_indices, counts = np.unique(spike_monitor.i, return_counts=True)\n",
    "            # Calculate rate (spikes / duration in seconds)\n",
    "            rates[neuron_indices] = counts / duration_sec\n",
    "        \n",
    "        # Discretize the rates to use as keys in the Q-table\n",
    "        # Simple discretization: binning rates (e.g., 0-10Hz, 10-30Hz, 30+Hz)\n",
    "        bins = [-np.inf, 10, 30, np.inf] # Define rate bins (adjust as needed)\n",
    "        discretized_rates = tuple(np.digitize(rates, bins))\n",
    "        \n",
    "        return discretized_rates\n",
    "\n",
    "    def step(self, action):\n",
    "        # In this simple task, the episode ends after one action\n",
    "        correct_action = self.pattern_labels[self.current_pattern_name]\n",
    "\n",
    "        if action == correct_action:\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            reward = -1.0\n",
    "\n",
    "        done = True\n",
    "        # Get the SNN output rates again to represent the 'next_state'\n",
    "        # In this specific task, the state doesn't change after the action,\n",
    "        # but we return it for consistency with RL loop. Can also return None.\n",
    "        next_state = self._get_snn_state() \n",
    "\n",
    "        return next_state, reward, done\n",
    "\n",
    "print(\"SNN Pattern Environment defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Q-Learning Agent (using the same class as before) ---\n",
    "env_snn = SNNPatternEnv(snn_network, patterns, pattern_labels, pattern_duration)\n",
    "\n",
    "# Test reset and state generation\n",
    "print(\"Testing environment reset...\")\n",
    "try:\n",
    "    test_state = env_snn.reset()\n",
    "    print(f\"Initial SNN state (discretized rates): {test_state}\")\n",
    "    print(f\"Presented pattern: {env_snn.current_pattern_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during env reset test: {e}\")\n",
    "    print(\"Check SNN setup and environment code.\")\n",
    "\n",
    "# Create the agent\n",
    "agent_snn = QLearningAgent(env_snn, alpha=0.1, gamma=0.9, # Gamma less important here (1-step episodes)\n",
    "                           epsilon=1.0, epsilon_decay=0.99, epsilon_min=0.05)\n",
    "print(\"\\nQ-Learning agent created for SNN environment.\")\n",
    "\n",
    "# --- Training Loop ---\n",
    "num_episodes_snn = 3000\n",
    "rewards_per_episode_snn = []\n",
    "history = [] # Store (pattern, chosen_action, correct_action)\n",
    "\n",
    "print(f\"\\nStarting SNN+RL training for {num_episodes_snn} episodes...\")\n",
    "start_time_snn = time.time()\n",
    "\n",
    "for episode in range(num_episodes_snn):\n",
    "    state = env_snn.reset()\n",
    "    # env.reset() runs the SNN and returns the discretized rate state\n",
    "\n",
    "    action = agent_snn.choose_action(state)\n",
    "    # The 'step' in this env mainly determines reward based on the action\n",
    "    next_state, reward, done = env_snn.step(action) \n",
    "\n",
    "    # Store history for analysis\n",
    "    correct_action = env_snn.pattern_labels[env_snn.current_pattern_name]\n",
    "    history.append({'pattern': env_snn.current_pattern_name, \n",
    "                      'state': state,\n",
    "                      'chosen': action, \n",
    "                      'correct': correct_action, \n",
    "                      'reward': reward})\n",
    "    \n",
    "    # Q-learning update uses the state observed *before* the action\n",
    "    agent_snn.update_q_table(state, action, reward, next_state, done) \n",
    "    agent_snn.update_epsilon()\n",
    "    rewards_per_episode_snn.append(reward)\n",
    "\n",
    "    if (episode + 1) % (num_episodes_snn // 10) == 0:\n",
    "        # Calculate accuracy over the last N episodes\n",
    "        recent_history = history[-(num_episodes_snn // 10):]\n",
    "        if len(recent_history) > 0:\n",
    "           recent_accuracy = sum(1 for h in recent_history if h['chosen'] == h['correct']) / len(recent_history)\n",
    "        else:\n",
    "           recent_accuracy = 0.0\n",
    "        print(f\"Episode {episode + 1}/{num_episodes_snn} | Recent Acc: {recent_accuracy:.3f} | Epsilon: {agent_snn.epsilon:.3f} | Q-States: {len(agent_snn.q_table)}\")\n",
    "\n",
    "end_time_snn = time.time()\n",
    "if num_episodes_snn > 0:\n",
    "    total_accuracy = sum(1 for h in history if h['chosen'] == h['correct']) / num_episodes_snn\n",
    "else:\n",
    "    total_accuracy = 0.0\n",
    "print(f\"\\nSNN+RL Training finished in {end_time_snn - start_time_snn:.2f} seconds.\")\n",
    "print(f\"Overall Accuracy: {total_accuracy:.3f}\")\n",
    "\n",
    "# --- Plot SNN+RL Training Progress ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "window_size = 100 \n",
    "# Calculate accuracy in windows\n",
    "accuracy_history = [1 if h['chosen'] == h['correct'] else 0 for h in history]\n",
    "if len(accuracy_history) >= window_size:\n",
    "    smoothed_accuracy = np.convolve(accuracy_history, np.ones(window_size)/window_size, mode='valid')\n",
    "    plot_indices = np.arange(window_size - 1, len(accuracy_history))\n",
    "    plt.plot(plot_indices, smoothed_accuracy)\n",
    "    plt.xlabel(f'Episode (Smoothed over {window_size})')\n",
    "else:\n",
    "    plt.plot(accuracy_history) # Plot raw if not enough data\n",
    "    plt.xlabel('Episode')\n",
    "\n",
    "plt.title('Accuracy over Time (SNN+RL)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(-0.05, 1.05)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Inspect Q-Table (Optional) ---\n",
    "print(f\"\\nQ-Table size: {len(agent_snn.q_table)} states encountered.\")\n",
    "print(\"Sample Q-Table entries:\")\n",
    "count = 0\n",
    "for state, actions in agent_snn.q_table.items():\n",
    "    # Format actions for better readability\n",
    "    action_values = {f\"Action {k}\": f\"{v:.2f}\" for k, v in actions.items()}\n",
    "    print(f\"  State {state}: {action_values}\")\n",
    "    count += 1\n",
    "    if count >= 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Discussion and Exercise 4 (30 mins)\n",
    "\n",
    "*   **Interpretation:** The SNN acted as a fixed, dynamic feature extractor. The Q-learner learned to map the SNN's output firing patterns (our discretized 'state') to the correct classification action.\n",
    "*   **Limitations:**\n",
    "    *   The SNN itself didn't learn; its weights were fixed. True neuromorphic RL would involve adapting the SNN's synapses based on reward (e.g., using reward-modulated plasticity).\n",
    "    *   The state discretization (binning/rounding firing rates) was basic and might lose information or lead to a large/sparse state space, potentially hindering learning. Using function approximation (like Deep Q-Networks, potentially with SNNs) is more scalable.\n",
    "    *   The task was simple (single step, binary classification).\n",
    "\n",
    "**Exercise 4: Explore the SNN-RL System:**\n",
    "\n",
    "1.  **SNN Output Neurons:** Change `num_outputs` in the SNN setup (e.g., to 2 or 8). Re-run the SNN setup, environment creation, agent creation, and training loop. How does this affect the size of the Q-table state space (number of unique discretized rate tuples) and the final accuracy? Does more complex SNN output help or hinder the simple Q-learner?\n",
    "2.  **SNN Connectivity/Weights:** Modify the SNN's connection probability (`connection_probability`) or the weight range (`min_weight_snn`, `max_weight_snn`). Re-run everything from the SNN setup onwards. Does a significantly different SNN structure make it harder or easier for the Q-learner to solve the task? (Remember the SNN weights are *not* learning here, so you're changing the fixed feature extractor).\n",
    "3.  **Pattern Similarity:** Make Pattern A and Pattern B more similar (e.g., change `high_rate_val` to be closer to `base_rate_val`, or make the active neuron groups overlap more). Re-run the pattern definition, environment, agent, and training. Can the system still distinguish them? How does accuracy change?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Exercise 4 Code Space ---\n",
    "\n",
    "# This exercise requires modifying parameters in the cells above and re-running them.\n",
    "# There's no single block of code to write here.\n",
    "\n",
    "# --- Your Actions ---\n",
    "\n",
    "# 1. Choose ONE parameter area to explore first (e.g., num_outputs).\n",
    "\n",
    "# 2. Go back to the relevant cell:\n",
    "#    - For num_outputs: '# --- SNN Setup ---' cell.\n",
    "#    - For connectivity/weights: '# --- SNN Setup ---' cell.\n",
    "#    - For pattern similarity: '# --- Define Spike Patterns ---' cell.\n",
    "\n",
    "# 3. Modify the parameter(s) in that cell.\n",
    "#    - Example (for num_outputs): Change `num_outputs = 4` to `num_outputs = 2`.\n",
    "#    - Example (for connectivity): Change `connection_probability = 0.5` to `0.2`.\n",
    "#    - Example (for weights): Change `min_weight_snn`, `max_weight_snn`.\n",
    "#    - Example (for patterns): Change `high_rate_val = 50` to `30`.\n",
    "\n",
    "# 4. Re-run the modified cell AND all subsequent cells in Module 4:\n",
    "#    - SNN Setup (if modified)\n",
    "#    - Pattern Definition (if modified)\n",
    "#    - SNN Pattern Environment Definition (re-run to pick up changes)\n",
    "#    - Agent Creation & Training Loop (re-run to train with new setup)\n",
    "\n",
    "# 5. Observe the results:\n",
    "#    - Look at the printed output during training (accuracy, Q-states encountered).\n",
    "#    - Examine the final accuracy plot.\n",
    "#    - Note the final Q-table size.\n",
    "\n",
    "# 6. Reflect: How did the change affect the system's ability to learn?\n",
    "#    - Did accuracy improve or decrease?\n",
    "#    - Did learning take longer (more episodes to reach good accuracy)?\n",
    "#    - Did the state space size (Q-States) change significantly?\n",
    "\n",
    "# 7. (Optional) Reset the parameter you changed, then try modifying a different one.\n",
    "\n",
    "print(\"--- Exercise 4 Instructions --- \")\n",
    "print(\"Modify parameters in the cells above as described in the markdown.\")\n",
    "print(\"Re-run the sequence of cells starting from your modification down to the plotting cell.\")\n",
    "print(\"Observe the impact on accuracy, learning speed, and Q-table size.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up and Further Learning (30 mins)\n",
    "\n",
    "Congratulations! You've covered the basics of Neuromorphic Computing and Reinforcement Learning, and even built a simple system combining them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Takeaways:\n",
    "*   Neuromorphic computing uses brain-inspired principles (spikes, parallelism, colocation) for efficient computation, especially with SNNs.\n",
    "*   SNNs communicate with timed spikes, simulated using models like LIF. Brian2 is a powerful tool for this.\n",
    "*   Reinforcement Learning trains agents to make decisions by maximizing rewards through environmental interaction (Q-Learning is a fundamental algorithm).\n",
    "*   Combining NC and RL holds promise for energy-efficient, biologically plausible AI, but presents challenges in learning rules and credit assignment.\n",
    "*   Even simple integrations (like using SNN outputs as RL states) demonstrate the potential synergy."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.14.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
